{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "#tqdm for the progress bar\n",
    "from tqdm import tqdm\n",
    "print(\"All modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the train.csv file from kaggle.\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load in data from kaggle\n",
    "train_X=pd.read_csv(\"~/Downloads/train.csv\")\n",
    "validation_X=train_X.loc[40000:]\n",
    "train_X=train_X.loc[:39999]\n",
    "train_y=train_X['label']\n",
    "validation_y=validation_X['label']\n",
    "del train_X['label']\n",
    "del validation_X['label']\n",
    "test_X=pd.read_csv(\"~/Downloads/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 784) (40000,) (2000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#convert to numpy arrays and normalize values between 0 and 1\n",
    "#normalizing allows the network to train better\n",
    "train_X=np.array(train_X)/255\n",
    "train_y=np.array(train_y)\n",
    "validation_X=np.array(validation_X)/255\n",
    "validation_y=np.array(validation_y)\n",
    "print(train_X.shape, train_y.shape, validation_X.shape, validation_y.shape)\n",
    "#test data\n",
    "test_X=np.array(test_X).astype(dtype='float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 10) (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "#convert to one-hot array\n",
    "train_y=np.array(pd.get_dummies(train_y))\n",
    "validation_y=np.array(pd.get_dummies(validation_y))\n",
    "print(train_y.shape, validation_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast_3:0' shape=(2000, 10) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make sure everything is a float32\n",
    "tf.cast(train_X, tf.float32)\n",
    "tf.cast(train_y, tf.float32)\n",
    "tf.cast(validation_X, tf.float32)\n",
    "tf.cast(validation_y, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up placeholders where data will be passed into  later\n",
    "features=tf.placeholder(tf.float32, shape=[None, 784])\n",
    "labels=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set some parameters\n",
    "batch_size=128\n",
    "\n",
    "nodes_hl1=1000\n",
    "nodes_hl2=500\n",
    "nodes_hl3=100\n",
    "\n",
    "output_size=10\n",
    "\n",
    "num_epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A website showing different weight initializations:\n",
    "https://intoli.com/blog/neural-network-initialization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create variables(weights and biases) Uses standard deviation of sqrt(2/nodes) which is a good starting point.\n",
    "\n",
    "weights_input_hl1=tf.get_variable('weights_input_hl1', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([784, nodes_hl1], dtype=tf.float32, stddev=np.sqrt(2/784)))\n",
    "biases_hl1=tf.get_variable('biases_hl1', [nodes_hl1], dtype=tf.float32, \n",
    "  initializer=tf.zeros_initializer)\n",
    "\n",
    "weights_hl1_hl2=tf.get_variable('weights_hl1_hl2', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([nodes_hl1, nodes_hl2], dtype=tf.float32, stddev=np.sqrt(2/nodes_hl1)))\n",
    "biases_hl2=tf.get_variable('biases_hl2', [nodes_hl2], dtype=tf.float32, \n",
    "  initializer=tf.zeros_initializer)\n",
    "\n",
    "weights_hl2_hl3=tf.get_variable('weights_hl2_hl3', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([nodes_hl2, nodes_hl3], dtype=tf.float32, stddev=np.sqrt(2/nodes_hl2)))\n",
    "biases_hl3=tf.get_variable('biases_hl3', [nodes_hl3], dtype=tf.float32, \n",
    "  initializer=tf.zeros_initializer)\n",
    "\n",
    "weights_hl3_output=tf.get_variable('weights_hl3_output', dtype=tf.float32, \n",
    "  initializer=tf.truncated_normal([nodes_hl3, output_size], dtype=tf.float32, stddev=np.sqrt(2/nodes_hl3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create saver, max_to_keep is maximum checkpoint files kept\n",
    "saver=tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropout rate, each time it is trained, ~20% of neurons will be killed in each layer, it helps prevent overfitting\n",
    "train_keep=0.8\n",
    "keep_amt=train_keep\n",
    "\n",
    "#training pass\n",
    "#elu=exponential linear unit, generally performs better than relu\n",
    "\n",
    "def forward_pass(x, keep_amt):\n",
    "    dropout_rate=tf.constant(keep_amt)\n",
    "    l1=tf.add(tf.matmul(x, weights_input_hl1), biases_hl1)\n",
    "    l1=tf.nn.elu(l1)\n",
    "    l1=tf.nn.dropout(l1, dropout_rate)\n",
    "    l2=tf.add(tf.matmul(l1, weights_hl1_hl2), biases_hl2)\n",
    "    l2=tf.nn.elu(l2)\n",
    "    l2=tf.nn.dropout(l2, dropout_rate)\n",
    "    l3=tf.add(tf.matmul(l2, weights_hl2_hl3), biases_hl3)\n",
    "    l3=tf.nn.elu(l3)\n",
    "    l3=tf.nn.dropout(l3, dropout_rate)\n",
    "    output_layer=tf.matmul(l3, weights_hl3_output)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cost and gradient descent\n",
    "#tf.reduce_mean=np.mean and tf.reduce_sum=np.sum\n",
    "lr=1e-3\n",
    "learning_rate=tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "logits=forward_pass(features,keep_amt)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#accuracy\n",
    "#argmax takes the maximum value in each vector and sets it to 1, all others are set to 0\n",
    "output=tf.nn.softmax(logits)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1)),tf.float32))\n",
    "\n",
    "#used later for predicting the test data\n",
    "prediction=tf.argmax(tf.nn.softmax(logits=output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No save file found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/30:  95%|███████████████████████████████████████████████████████████   | 298/313 [00:08<00:00, 36.31batches/s]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "before_time=time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #restore weights if file found\n",
    "    try:\n",
    "        saver.restore(sess, os.path.join(cwd,\"weights/model.ckpt\"))\n",
    "        print(\"Model restored.\")\n",
    "    except:\n",
    "        print(\"No save file found.\")\n",
    "\n",
    "    \n",
    "    batch_count = int(math.ceil(len(train_X)/batch_size))\n",
    "    best_val_acc=0\n",
    "    last_improve_epoch=0\n",
    "    for epoch in range(num_epochs):\n",
    "        #shuffle data\n",
    "        state=np.random.get_state()\n",
    "        np.random.shuffle(train_X)\n",
    "        np.random.set_state(state)\n",
    "        np.random.shuffle(train_y)\n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch+1, num_epochs), unit='batches')\n",
    "        train_loss=0.0\n",
    "        # The training cycle\n",
    "        keep_amt=train_keep\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_X[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_y[batch_start:batch_start + batch_size]\n",
    "            #train\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={features: batch_features, labels: batch_labels, learning_rate:lr})\n",
    "            train_loss+=c\n",
    "        #set keep amount to 100% for testing\n",
    "        keep_amt=1.0    \n",
    "        validation_accuracy=sess.run(accuracy, feed_dict={features: validation_X, labels: validation_y})\n",
    "        print('Training Loss = {}, Validation Accuracy = {}'.format(train_loss, validation_accuracy))\n",
    "\n",
    "        #save model if validation accuracy is at a new best\n",
    "        if validation_accuracy>best_val_acc:\n",
    "            save_path = saver.save(sess, os.path.join(cwd,\"weights/model.ckpt\"))\n",
    "            print(\"Model saved in file: {}\".format(save_path))\n",
    "            print(\"Accuracy improved from {} to {}\".format(best_val_acc, validation_accuracy))\n",
    "            best_val_acc=validation_accuracy\n",
    "            last_improve_epoch=epoch\n",
    "        #if model hasn't improved for 5 epochs step down learning rate\n",
    "        elif (epoch-last_improve_epoch)%5==0:\n",
    "            lr/=5\n",
    "            print(\"Learning rate decreased to {}.\".format(lr))\n",
    "            \n",
    "    print(\"Training Finished! It took {} minutes. Best validation accuracy: {}\"\n",
    "          .format(np.round((time.time()-before_time)/60,2), best_val_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
